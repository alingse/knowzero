# KnowZero è®¾è®¡å†²çªåˆ†æä¸è§£å†³æ–¹æ¡ˆ

> å¯¹ 10 ä¸ªæ ¸å¿ƒå†²çªçš„æ·±åº¦åˆ†æå’Œè§£å†³ç­–ç•¥

---

## å†²çªæ€»è§ˆ

| # | å†²çªé¢†åŸŸ | ä¸¥é‡ç¨‹åº¦ | ç±»å‹ |
|---|---------|---------|------|
| 1 | æ¶æ„ç‰ˆæœ¬å†²çª | ğŸ”´ é«˜ | æ¶æ„ä¸ä¸€è‡´ |
| 2 | æŒä¹…åŒ–æ€§èƒ½ | ğŸ”´ é«˜ | æ€§èƒ½é£é™© |
| 3 | å®ä½“è¯é—­ç¯ | ğŸŸ¡ ä¸­ | åŠŸèƒ½ç¼ºå¤± |
| 4 | æ–‡æ¡£æ›´æ–°å†²çª | ğŸŸ¡ ä¸­ | ç”¨æˆ·ä½“éªŒ |
| 5 | ç›®å½•åˆ†ç±»æ··ä¹± | ğŸŸ¡ ä¸­ | å¯ç»´æŠ¤æ€§ |
| 6 | æµå¼ä¸æŒä¹…åŒ– | ğŸ”´ é«˜ | æ•°æ®ä¸€è‡´æ€§ |
| 7 | æ„å›¾åˆ†ç±»è¿‡åº¦ | ğŸŸ¢ ä½ | æ€§èƒ½ä¼˜åŒ– |
| 8 | ç”¨æˆ·çŠ¶æ€ä¸ä¸€è‡´ | ğŸŸ¡ ä¸­ | ä½“éªŒä¸€è‡´æ€§ |
| 9 | é”šç‚¹å¤±æ•ˆ | ğŸ”´ é«˜ | åŠŸèƒ½å¯é æ€§ |
| 10 | å‘é‡åŒæ­¥ç¼ºå¤± | ğŸŸ¢ ä½ | åŠŸèƒ½å®Œæ•´ |

---

## å†²çªä¸€ï¼šæ¶æ„ç‰ˆæœ¬å†²çª (ğŸ”´ é«˜)

### é—®é¢˜æè¿°

```
v1: åŸºäº complexity çš„ç®€å•è·¯ç”±
     â†“
     "simple" â†’ ç›´æ¥åˆ° Content Agent

v2: Input Normalizer ç»Ÿä¸€å¤„ç†
     â†“
     æ‰€æœ‰è¾“å…¥éƒ½èµ° Intent â†’ Route â†’ Content
```

**é£é™©**ï¼šå¦‚æœåŒæ—¶ä¿ç•™ä¸¤ç§é€»è¾‘ï¼Œä¼šå¯¼è‡´ä¸å¯é¢„æµ‹çš„è¡Œä¸ºã€‚

### è§£å†³æ–¹æ¡ˆ

**å†³ç­–ï¼šåºŸå¼ƒ v1 ç®€å•è·¯ç”±ï¼Œå…¨é¢é‡‡ç”¨ v2**

```python
# backend/agent/graph.py - ç»Ÿä¸€æ¶æ„

def create_knowzero_graph():
    """
    åˆ›å»ºç»Ÿä¸€çš„ Agent Graph

    v2 æ¶æ„æˆä¸ºå”¯ä¸€æ¶æ„
    """

    graph = StateGraph(AgentState)

    # ç»Ÿä¸€çš„è¾“å…¥å¤„ç†èŠ‚ç‚¹
    graph.add_node("input_normalizer", input_normalizer_node)
    graph.add_node("intent_agent", intent_agent_node)
    graph.add_node("route_agent", route_agent_node)
    graph.add_node("content_agent", content_agent_node)

    # è®¾ç½®å…¥å£
    graph.set_entry_point("input_normalizer")

    # ç»Ÿä¸€çš„æ¡ä»¶è¾¹
    graph.add_conditional_edges(
        "intent_agent",
        route_by_intent,
        {
            # ç§»é™¤ "simple" å¿«é€Ÿé€šè·¯
            # æ‰€æœ‰æ„å›¾éƒ½ç»è¿‡ Route Agent
            "generate": "route_agent",
            "follow_up": "route_agent",
            "optimize": "route_agent",
            "navigate": "navigator_node"
        }
    )

    return compiled
```

**çŠ¶æ€å®šä¹‰åŒæ­¥**

```python
# backend/agent/state.py - å”¯ä¸€çŠ¶æ€å®šä¹‰

# åºŸå¼ƒ v1 çš„ç®€å•å­—æ®µï¼Œç»Ÿä¸€ä½¿ç”¨ v2 çš„ç»“æ„
class AgentState(TypedDict):
    """ç»Ÿä¸€çš„ Agent çŠ¶æ€"""

    # === è¾“å…¥ ===
    input_source: str  # v2: InputSource enum
    raw_message: str
    comment_data: Optional[dict]
    entity_data: Optional[dict]

    # === Agent è¾“å‡º ===
    intent: Optional[dict]  # ç»Ÿä¸€æ ¼å¼
    routing_decision: Optional[dict]  # ç»Ÿä¸€æ ¼å¼

    # ä¸å†å•ç‹¬çš„ complexity å­—æ®µ - ç»Ÿä¸€ç”± Route Agent åˆ¤æ–­
```

**è¿ç§»è®¡åˆ’**ï¼š

```markdown
## v1 â†’ v2 è¿ç§»æ¸…å•

### Agent èŠ‚ç‚¹
- [ ] ç§»é™¤ simple è·¯ç”±é€»è¾‘
- [ ] æ‰€æœ‰è¾“å…¥ç»è¿‡ Input Normalizer
- [ ] Intent Agent ç»Ÿä¸€è¾“å‡ºæ ¼å¼
- [ ] Route Agent å¤„ç†æ‰€æœ‰è¾“å…¥ç±»å‹

### State å®šä¹‰
- [ ] ç»Ÿä¸€ AgentState ç»“æ„
- [ ] ç§»é™¤åºŸå¼ƒå­—æ®µ
- [ ] æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨æ–°çŠ¶æ€

### æ–‡æ¡£
- [ ] åˆ é™¤ agent-arch-v1.md
- [ ] æ›´æ–° agent-arch-v2.md ä¸ºæ¶æ„ä¸»æ–‡æ¡£
- [ ] æ·»åŠ è¿ç§»æŒ‡å—
```

---

## å†²çªäºŒï¼šCheckpoint è†¨ç‚¸ (ğŸ”´ é«˜)

### é—®é¢˜æè¿°

```
æ¯æ¬¡ invoke:
State.messages (1000 æ¡æ¶ˆæ¯)
    â†“
åºåˆ—åŒ–åˆ° checkpoint
    â†“
SQLite checkpoint è¡¨å¢é•¿
```

**é£é™©**ï¼š
- 1000 æ¡æ¶ˆæ¯ Ã— 500 å­—ç¬¦ â‰ˆ 500KB æ¯ä¸ª checkpoint
- 100 æ¬¡å¯¹è¯ = 50MB checkpoint æ•°æ®

### è§£å†³æ–¹æ¡ˆï¼šåˆ†å±‚å­˜å‚¨ç­–ç•¥

```python
# backend/checkpoint/layered_saver.py

class LayeredCheckpointSaver:
    """
    åˆ†å±‚æ£€æŸ¥ç‚¹ä¿å­˜å™¨

    - å®Œæ•´çŠ¶æ€ï¼šæœ€è¿‘ N ä¸ª
    - ç²¾ç®€çŠ¶æ€ï¼šæ›´æ—©çš„åªä¿ç•™æ‘˜è¦
    """

    FULL_WINDOW = 20  # æœ€è¿‘ 20 æ¡å®Œæ•´ä¿å­˜
    SUMMARY_WINDOW = 50  # 21-50 æ¡ä¿å­˜æ‘˜è¦
    ARCHIVE_THRESHOLD = 100  # è¶…è¿‡ 100 æ¡å½’æ¡£

    def put(self, config, checkpoint, metadata):
        """æ™ºèƒ½åˆ†å±‚ä¿å­˜"""

        thread_id = config["configurable"]["thread_id"]
        messages = checkpoint.get("channel_values", {}).get("messages", [])
        message_count = len(messages)

        # === å±‚ 1: æœ€è¿‘ 20 æ¡ ===
        if message_count <= self.FULL_WINDOW:
            # ä¿å­˜å®Œæ•´ checkpoint
            return self._save_full_checkpoint(config, checkpoint, metadata)

        # === å±‚ 2: 21-50 æ¡ ===
        elif message_count <= self.SUMMARY_WINDOW:
            # åªä¿å­˜æ‘˜è¦
            summary_checkpoint = self._create_summary_checkpoint(checkpoint)
            return self._save_summary_checkpoint(config, summary_checkpoint, metadata)

        # === å±‚ 3: è¶…è¿‡ 50 æ¡ ===
        else:
            # å½’æ¡£æ—§æ¶ˆæ¯ï¼Œåªä¿å­˜æ–°çš„
            return self._archive_and_save_new(config, checkpoint, metadata)

    def _create_summary_checkpoint(self, checkpoint: dict) -> dict:
        """åˆ›å»ºç²¾ç®€çš„æ‘˜è¦ checkpoint"""

        messages = checkpoint.get("channel_values", {}).get("messages", [])
        recent = messages[-10:]  # æœ€è¿‘ 10 æ¡å®Œæ•´
        older = messages[:-10]  # æ›´æ—©çš„æ€»ç»“

        # ä½¿ç”¨ message-management.md ä¸­çš„æ€»ç»“å™¨
        summary = summarize_messages(older)

        return {
            **checkpoint,
            "id": str(uuid.uuid4()),
            "channel_values": {
                **checkpoint.get("channel_values", {}),
                "messages": recent + [
                    {"role": "system", "content": f"[å†å²æ€»ç»“] {summary}"}
                ]
            },
            "metadata": {
                **checkpoint.get("metadata", {}),
                "storage_mode": "summary",  # æ ‡è®°ä¸ºæ‘˜è¦æ¨¡å¼
                "compressed_count": len(older)
            }
        }
```

**é…åˆ LangGraph ä½¿ç”¨**ï¼š

```python
# ä½¿ç”¨åˆ†å±‚ Saver
checkpointer = LayeredCheckpointSaver(conn_str="sqlite:///knowzero.db")

graph = graph.compile(checkpointer=checkpointer)

# checkpoint ä¼šè‡ªåŠ¨æ ¹æ®æ¶ˆæ¯æ•°é‡é€‰æ‹©å­˜å‚¨æ–¹å¼
```

---

## å†²çªä¸‰ï¼šå®ä½“è¯ç‚¹å‡»é—­ç¯ (ğŸŸ¡ ä¸­)

### é—®é¢˜æè¿°

```
å®ä½“è¯åªåœ¨ç”Ÿæˆæ—¶æå–
     â†“
ç”¨æˆ·ç‚¹å‡»å®ä½“è¯ â†’ åªèƒ½æ–°å»ºæ–‡æ¡£
     â†“
æ— æ³•ï¼šæ›´æ–°ç°æœ‰å®ä½“è¯æ–‡æ¡£ã€å…³è”åˆ°å¤šä¸ªçˆ¶æ–‡æ¡£
```

**é£é™©**ï¼šçŸ¥è¯†å›¾è°±ä¸­å¯èƒ½å‡ºç°é‡å¤å†…å®¹ã€‚

### è§£å†³æ–¹æ¡ˆï¼šå®ä½“è¯ç´¢å¼•ç³»ç»Ÿ

```python
# backend/services/entity_index.py

class EntityIndex:
    """
    å®ä½“è¯ç´¢å¼•ç³»ç»Ÿ

    1. å®ä½“è¯ç‹¬ç«‹äºæ–‡æ¡£å­˜åœ¨
    2. æ”¯æŒå¤šå¯¹å¤šå…³ç³»
    3. æ”¯æŒåˆå¹¶/æ›´æ–°
    """

    async def get_or_create_entity(self, name: str, session_id: str) -> dict:
        """è·å–æˆ–åˆ›å»ºå®ä½“è¯"""

        # æœç´¢å®ä½“è¯
        entity = await self.db.get_entity_by_name(name)

        if entity:
            # è¿”å›ç°æœ‰å®ä½“è¯ï¼ˆåŒ…å«æ‰€æœ‰å…³ç³»ï¼‰
            return {
                "id": entity["id"],
                "name": entity["name"],
                "documents": entity["documents"],  # æ‰€æœ‰åŒ…å«æ­¤å®ä½“çš„æ–‡æ¡£
                "is_new": False
            }

        # åˆ›å»ºæ–°å®ä½“è¯
        entity_id = await self.db.create_entity(
            name=name,
            session_id=session_id,
            type="concept"  # concept, tool, library, etc.
        )

        return {
            "id": entity_id,
            "name": name,
            "documents": [],
            "is_new": True
        }

    async def link_entity_to_document(
        self, entity_id: int, doc_id: int, link_type: str
    ):
        """
        å…³è”å®ä½“è¯åˆ°æ–‡æ¡£

        link_type: "explains" | "mentions" | "related"
        """

        await self.db.create_entity_link(
            entity_id=entity_id,
            document_id=doc_id,
            link_type=link_type
        )

    async def merge_entities(self, source_id: int, target_id: int):
        """
        åˆå¹¶é‡å¤çš„å®ä½“è¯

        å½“å‘ç°ä¸¤ä¸ªå®ä½“è¯å®é™…æ˜¯åŒä¸€æ¦‚å¿µæ—¶
        """

        # 1. è·å–ä¸¤ä¸ªå®ä½“è¯çš„æ‰€æœ‰å…³è”
        source_links = await self.db.get_entity_links(source_id)
        target_links = await self.db.get_entity_links(target_id)

        # 2. å°†ç›®æ ‡å®ä½“çš„å…³è”è¿ç§»åˆ°æºå®ä½“
        for link in target_links:
            await self.db.create_entity_link(
                entity_id=source_id,
                document_id=link["document_id"],
                link_type=link["link_type"]
            )

        # 3. åˆ é™¤ç›®æ ‡å®ä½“è¯
        await self.db.delete_entity(target_id)

        # 4. æ›´æ–°æ‰€æœ‰æ–‡æ¡£çš„å¼•ç”¨
        await self.db.remap_entity_references(target_id, source_id)
```

**æ•°æ®æ¨¡å‹**ï¼š

```sql
-- ============================================
-- å®ä½“è¯è¡¨ (ç‹¬ç«‹å­˜åœ¨)
-- ============================================
CREATE TABLE entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT UNIQUE NOT NULL,
    session_id TEXT NOT NULL,

    -- åˆ†ç±»
    type TEXT,  -- concept, tool, library, technique
    category TEXT,  -- å¯é€‰çš„å¤šçº§åˆ†ç±»

    -- çŠ¶æ€
    status TEXT DEFAULT 'active',  -- active, merged, deprecated

    FOREIGN KEY (session_id) REFERENCES sessions(id)
);

-- ============================================
-- å®ä½“è¯-æ–‡æ¡£å…³è”è¡¨ (å¤šå¯¹å¤š)
-- ============================================
CREATE TABLE entity_document_links (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    entity_id INTEGER NOT NULL,
    document_id INTEGER NOT NULL,
    link_type TEXT NOT NULL,  -- 'explains', 'mentions', 'related'

    -- å…³è”çš„å…ƒæ•°æ®
    context_snippet TEXT,  -- æ–‡æ¡£ä¸­å¦‚ä½•æåˆ°è¿™ä¸ªå®ä½“
    confidence FLOAT,  -- AI åˆ¤æ–­çš„ç½®ä¿¡åº¦

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (entity_id) REFERENCES entities(id),
    FOREIGN KEY (document_id) REFERENCES documents(id),
    UNIQUE(entity_id, document_id, link_type)
);

-- ============================================
-- æ–‡æ¡£ä¸­æåˆ°çš„å®ä½“è¯ (ç”¨äºæå–)
-- ============================================
CREATE TABLE document_entities (
    document_id INTEGER NOT NULL,
    entity_id INTEGER NOT NULL,

    -- ä½ç½®ä¿¡æ¯
    position_start INTEGER,
    position_end INTEGER,
    context TEXT,  -- å‘¨å›´æ–‡æœ¬

    PRIMARY KEY (document_id, entity_id),
    FOREIGN KEY (document_id) REFERENCES documents(id),
    FOREIGN KEY (entity_id) REFERENCES entities(id)
);
```

**äº¤äº’æ›´æ–°**ï¼š

```
ç”¨æˆ·ç‚¹å‡»å®ä½“è¯
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Entity Index æŸ¥è¯¢                                      â”‚
â”‚                                                          â”‚
â”‚  å®ä½“è¯å­˜åœ¨ï¼Ÿ                                              â”‚
â”‚    â”œâ”€ Yes â†’ æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£                              â”‚
â”‚  â”‚         â””â”€ ç”¨æˆ·é€‰æ‹©æŸ¥çœ‹æˆ–æ›´æ–°                         â”‚
â”‚  â”‚                                                      â”‚
â”‚  â””â”€ No â†’ Content Agent ç”Ÿæˆæ–‡æ¡£                     â”‚
â”‚              å¹¶å…³è”åˆ°æ–°æ–‡æ¡£                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å†²çªå››ï¼šæ–‡æ¡£æ›´æ–°å†²çª (ğŸŸ¡ ä¸­)

### é—®é¢˜æè¿°

```
ç”¨æˆ·å¯¹åŒä¸€ç« èŠ‚å¤šæ¬¡è¯„è®º"å¤ªæŠ½è±¡"
     â†“
rewrite: é‡å†™æ•´ä¸ªç« èŠ‚ (æ—§è¯„è®ºå¤±æ•ˆ)
append: ä¸æ–­è¿½åŠ  (æ–‡æ¡£å†—é•¿)
```

**é£é™©**ï¼šç”¨æˆ·ä½“éªŒä¸ä¸€è‡´ï¼Œæ–‡æ¡£å¯èƒ½å˜å¾—å†—é•¿æˆ–å†…å®¹ä¸¢å¤±ã€‚

### è§£å†³æ–¹æ¡ˆï¼šå¢é‡æ›´æ–°ç³»ç»Ÿ

```python
# backend/services/document_updater.py

class DocumentUpdater:
    """
    æ–‡æ¡£å¢é‡æ›´æ–°ç³»ç»Ÿ

    1. è¯„è®ºé”šç‚¹ä½¿ç”¨è¯­ä¹‰å®šä½ (ä¸ä¾èµ–å­—ç¬¦åç§»)
    2. æ”¯æŒæ®µè½çº§åˆ«æ›´æ–°
    3. å˜æ›´å†å²å¯è¿½è¸ª
    """

    async def optimize_section(
        self, document_id: int, comment: Comment
    ) -> UpdateResult:
        """
        ä¼˜åŒ–æ–‡æ¡£ç« èŠ‚

        åˆ†ææœ€ä½³æ›´æ–°ç­–ç•¥å¹¶æ‰§è¡Œ
        """

        # 1. åˆ†ææ›´æ–°ç±»å‹
        strategy = await self._analyze_update_strategy(document_id, comment)

        # 2. æ‰§è¡Œæ›´æ–°
        result = await self._apply_strategy(strategy, document_id, comment)

        # 3. è®°å½•å˜æ›´
        await self._record_change(document_id, result)

        return result

    async def _analyze_update_strategy(
        self, document_id: int, comment: Comment
    ) -> str:
        """
        åˆ†ææ›´æ–°ç­–ç•¥

        æ™ºèƒ½å†³å®šï¼šè¿½åŠ  vs é‡å†™ vs æ’å…¥
        """

        # è·å–ç›¸å…³å†å²
        history = await self.db.get_recent_updates(document_id, limit=10)

        # æ£€æµ‹æ¨¡å¼
        recent_similar = [h for h in history if h.type == comment.type]

        if len(recent_similar) >= 3:
            # ç”¨æˆ·è¿ç»­3æ¬¡ç±»ä¼¼è¯„è®º â†’ é‡å†™
            return "rewrite_section"

        elif comment.position and comment.position_start:
            # æœ‰ä½ç½®ä¿¡æ¯ â†’ æ’å…¥åˆ°æŒ‡å®šä½ç½®
            return "insert_at_position"

        else:
            # è¿½åŠ åˆ°ç« èŠ‚æœ«å°¾
            return "append_to_section"

    async def _apply_strategy(
        self, strategy: str, document_id: int, comment: Comment
    ) -> dict:
        """åº”ç”¨æ›´æ–°ç­–ç•¥"""

        if strategy == "rewrite_section":
            return await self._rewrite_section(document_id, comment)

        elif strategy == "insert_at_position":
            return await self._insert_at_position(document_id, comment)

        elif strategy == "append_to_section":
            return await self._append_to_section(document_id, comment)

    async def _rewrite_section(self, document_id: int, comment: Comment):
        """é‡å†™ç« èŠ‚"""

        # ä¿å­˜ç‰ˆæœ¬
        await self.db.save_document_version(document_id)

        # é‡å†™æŒ‡å®šç« èŠ‚
        document = await self.db.get_document(document_id)
        section_id = comment.section_id

        optimized_content = await self.llm.generate_rewrite(
            section=document.get_section(section_id),
            user_comment=comment.comment,
            keep_structure=True
        )

        document.update_section(section_id, optimized_content)
        await self.db.save_document(document)

        return {
            "strategy": "rewrite",
            "new_version": document.version + 1,
            "preserved_anchors": []  # é‡å†™åæ—§çš„é”šç‚¹å¤±æ•ˆ
        }
```

**é”šç‚¹è§£å†³æ–¹æ¡ˆ - è¯­ä¹‰é”šç‚¹**ï¼š

```python
# backend/models/anchors.py

class SemanticAnchor:
    """
    è¯­ä¹‰é”šç‚¹ç³»ç»Ÿ

    ä¸ä¾èµ–å­—ç¬¦åç§»ï¼Œä½¿ç”¨ç¨³å®šçš„æ ‡è¯†ç¬¦
    """

    def __init__(self):
        self.anchor_counter = 0

    def create_anchor(self, text: str, context: str) -> str:
        """ä¸ºæ–‡æœ¬æ®µåˆ›å»ºç¨³å®šçš„é”šç‚¹"""

        self.anchor_counter += 1

        # ä½¿ç”¨æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œä½œä¸ºé”šç‚¹åŸºç¡€
        content_hash = hashlib.md5(text.encode()).hexdigest()[:8]

        return f"anchor-{content_hash}-{self.anchor_counter}"

    def locate_anchor(self, document: dict, anchor_id: str) -> dict:
        """
        åœ¨æ–‡æ¡£ä¸­å®šä½é”šç‚¹

        å³ä½¿æ–‡æ¡£è¢«é‡å†™ï¼Œä¹Ÿèƒ½é€šè¿‡å†…å®¹ç›¸ä¼¼åº¦æ‰¾åˆ°
        """

        # æœç´¢åŒ…å«é”šç‚¹å†…å®¹çš„æ®µè½
        for section in document.sections:
            if self._text_contains(section.content, anchor_id):
                return {
                    "section_id": section.id,
                    "confidence": 1.0
                }

        # å¦‚æœå®Œå…¨åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨å‘é‡æœç´¢
        return self._fuzzy_locate_anchor(document, anchor_id)
```

---

## å†²çªäº”ï¼šç›®å½•åˆ†ç±»æ··ä¹± (ğŸŸ¡ ä¸­)

### é—®é¢˜æè¿°

```
AI æ¯æ¬¡åˆ†ç±»ç»´åº¦ä¸ä¸€è‡´
     â†“
ç›®å½•æ ‘å˜å¾—æ··ä¹±
```

**é£é™©**ï¼šå¯ç»´æŠ¤æ€§å·®ï¼Œç”¨æˆ·ä½“éªŒå·®ã€‚

### è§£å†³æ–¹æ¡ˆï¼šåˆ†ç±» Schema ç³»ç»Ÿ

```python
# backend/services/taxonomy.py

class TaxonomySchema:
    """
    åˆ†ç±»æ¨¡å¼ç³»ç»Ÿ

    å®šä¹‰å…¨å±€çš„åˆ†ç±»ç»´åº¦å’Œè§„åˆ™
    """

    # === é¢„å®šä¹‰åˆ†ç±»ç»´åº¦ ===
    DIMENSIONS = {
        "domain": ["å‰ç«¯", "åç«¯", "ç§»åŠ¨ç«¯", "æ•°æ®åº“", "ç®—æ³•"],
        "frontend_framework": ["React", "Vue", "Angular", "Svelte"],
        "backend_language": ["Python", "Go", "Java", "Node.js"],
        "database": ["PostgreSQL", "MySQL", "MongoDB", "Redis"],
        "concept_level": ["å…¥é—¨", "åŸºç¡€", "è¿›é˜¶", "å®æˆ˜"]
    }

    # === åˆ†ç±»è§„åˆ™ ===
    RULES = {
        "consistency": "åŒä¸€ä¼šè¯å†…ï¼ŒåŒä¸€ä¸»é¢˜çš„æ–‡æ¡£å¿…é¡»ä½¿ç”¨ç›¸åŒåˆ†ç±»",
        "inheritance": "å­æ–‡æ¡£é»˜è®¤ç»§æ‰¿çˆ¶æ–‡æ¡£çš„é¢†åŸŸ",
        "max_depth": "ç›®å½•æ ‘æœ€å¤§æ·±åº¦ä¸º 4 å±‚",
        "leaf_preference": "å†…å®¹ç±»æ–‡æ¡£ä¼˜å…ˆä½œä¸ºå¶å­èŠ‚ç‚¹"
    }

    def validate_path(self, path: str) -> dict:
        """éªŒè¯åˆ†ç±»è·¯å¾„æ˜¯å¦åˆæ³•"""

        parts = path.split('/')

        result = {
            "valid": True,
            "errors": [],
            "normalized_path": path
        }

        # æ£€æŸ¥æ·±åº¦
        if len(parts) > self.RULES["max_depth"]:
            result["valid"] = False
            result["errors"].append(f"ç›®å½•æ·±åº¦ {len(parts)} è¶…è¿‡æœ€å¤§ {self.RULES['max_depth']}")

        # æ£€æŸ¥ç»´åº¦ä¸€è‡´æ€§
        current_domain = None
        for i, part in enumerate(parts):
            if part in self.DIMENSIONS["domain"]:
                if current_domain is None:
                    current_domain = part
                elif current_domain != part:
                    result["valid"] = False
                    result["errors"].append(f"ç¬¬ {i+1} å±‚æ··ç”¨é¢†åŸŸ: {current_domain} â†’ {part}")

        return result

    def suggest_category(self, topic: str, context: dict) -> str:
        """
        å»ºè®®åˆ†ç±»è·¯å¾„

        è€ƒè™‘ï¼šå·²æœ‰åˆ†ç±»ã€ç”¨æˆ·åå¥½ã€ä¸»é¢˜å†…å®¹
        """

        # 1. åˆ†æä¸»é¢˜å…³é”®è¯
        keywords = self._extract_keywords(topic)

        # 2. æ£€æŸ¥å†å²åˆ†ç±»
        history = context.get("classification_history", {})

        # 3. åº”ç”¨è§„åˆ™
        suggested = self._apply_rules(keywords, history)

        return suggested

    def _extract_keywords(self, topic: str) -> dict:
        """ä»ä¸»é¢˜æå–å…³é”®è¯"""

        # ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
        # åœ¨ç”Ÿäº§ä¸­å¯ä»¥ç”¨ LLM

        tech_keywords = {
            "React": {"domain": "å‰ç«¯", "framework": "React"},
            "Python": {"domain": "åç«¯", "language": "Python"},
            "SQL": {"domain": "æ•°æ®åº“", "technology": "SQL"},
            # ... æ›´å¤š
        }

        for tech, info in tech_keywords.items():
            if tech in topic:
                return info

        return {"domain": "æœªåˆ†ç±»", "framework": "é€šç”¨"}

    def _apply_rules(self, keywords: dict, history: dict) -> str:
        """åº”ç”¨åˆ†ç±»è§„åˆ™"""

        domain = keywords.get("domain", "æœªåˆ†ç±»")
        framework = keywords.get("framework", "")

        # æ„å»ºè·¯å¾„
        if domain == "å‰ç«¯":
            if framework:
                return f"{domain}/{framework}/æ¦‚å¿µ"
            else:
                return f"{domain}/é€šç”¨"

        elif domain == "åç«¯":
            if framework:
                return f"{domain}/{framework}/æ ‡å‡†åº“"
            else:
                return f"{domain}/é€šç”¨"

        return f"æœªåˆ†ç±»/{framework}"
```

**ä½¿ç”¨æ–¹å¼**ï¼š

```python
# backend/agent/nodes.py

async def content_agent_node(state: AgentState):
    """Content Agent - ä½¿ç”¨åˆ†ç±» Schema"""

    document = state.get("document", {})

    # ä½¿ç”¨ Taxonomy éªŒè¯å’Œè§„èŒƒåˆ†ç±»
    taxonomy = TaxonomySchema()
    suggested_path = taxonomy.suggest_category(
        topic=document["topic"],
        context={
            "session_id": state["session_id"],
            "classification_history": state.get("taxonomy_history", {})
        }
    )

    # éªŒè¯è·¯å¾„
    validation = taxonomy.validate_path(suggested_path)

    if not validation["valid"]:
        # è·¯å¾„ä¸åˆæ³•ï¼Œä½¿ç”¨é»˜è®¤
        suggested_path = f"æœªåˆ†ç±»/{document['topic']}"

    # æ›´æ–°æ–‡æ¡£
    document["category_path"] = validation["normalized_path"]

    return {**state, "document": document}
```

---

## å†²çªå…­ï¼šæµå¼ä¸æŒä¹…åŒ–è„±èŠ‚ (ğŸ”´ é«˜)

### é—®é¢˜æè¿°

```
Agent ç”Ÿæˆæ–‡æ¡£ (æµå¼è¾“å‡º)
     â†“
ç”¨æˆ·åˆ·æ–° / ç½‘ç»œä¸­æ–­
     â†“
Checkpoint è¿˜æ²¡ä¿å­˜ â†’ å†…å®¹ä¸¢å¤±
```

**é£é™©**ï¼šæ•°æ®ä¸¢å¤±ï¼Œç”¨æˆ·ä½“éªŒå·®ã€‚

### è§£å†³æ–¹æ¡ˆï¼šæµå¼ä¸­é—´æ€ä¿å­˜

```python
# backend/checkpoint/streaming_saver.py

from langgraph.checkpoint import Checkpoint
import asyncio

class StreamingCheckpointSaver:
    """
    æµå¼æ£€æŸ¥ç‚¹ä¿å­˜å™¨

    åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®šæœŸä¿å­˜ä¸­é—´çŠ¶æ€
    """

    def __init__(self, base_saver):
        self.base_saver = base_saver
        self.pending_checkpoints = {}  # thread_id -> pending checkpoint
        self.save_interval = 3  # æ¯ 3 ç§’ä¿å­˜ä¸€æ¬¡

    async def save_intermediate(
        self, thread_id: str, node_name: str, partial_state: dict
    ):
        """ä¿å­˜ä¸­é—´çŠ¶æ€"""

        checkpoint = Checkpoint(
            id=str(uuid.uuid4()),
            channel_values={
                **partial_state.get("channel_values", {}),
                "_streaming": True,  # æ ‡è®°ä¸ºæµå¼ä¸­
                "_last_update": datetime.now().isoformat()
            },
            metadata={
                "source": node_name,
                "is_partial": True  # æ ‡è®°ä¸ºä¸å®Œæ•´
            }
        )

        self.pending_checkpoints[thread_id] = checkpoint

    async def finalize_checkpoint(
        self, thread_id: str, final_state: dict
    ):
        """å®Œæˆæ£€æŸ¥ç‚¹"""

        # 1. è·å–å¾…ä¿å­˜çš„ä¸­é—´æ€
        pending = self.pending_checkpoints.get(thread_id)

        # 2. åˆ›å»ºæœ€ç»ˆæ£€æŸ¥ç‚¹
        final_checkpoint = Checkpoint(
            id=str(uuid.uuid4()),
            channel_values=final_state.get("channel_values", {}),
            metadata=final_state.get("metadata", {})
        )

        # 3. åˆ é™¤ä¸­é—´æ€å¹¶ä¿å­˜æœ€ç»ˆæ€
        if pending:
            del self.pending_checkpoints[thread_id]

        return await self.base_saver.put(
            {"configurable": {"thread_id": thread_id}},
            final_checkpoint,
            {}
        )
```

**é…åˆ LangGraph ä½¿ç”¨**ï¼š

```python
# backend/agent/streaming_content.py

class StreamingContentAgent:
    """æ”¯æŒæµå¼ä¸­é—´æ€ä¿å­˜çš„å†…å®¹ç”Ÿæˆå™¨"""

    def __init__(self, llm, checkpoint_saver):
        self.llm = llm
        self.checkpoint_saver = checkpoint_saver

    async def generate_with_checkpoints(
        self, topic: str, config: dict
    ) -> dict:
        """ç”Ÿæˆæ–‡æ¡£å¹¶å®šæœŸä¿å­˜ä¸­é—´æ€"""

        thread_id = config["configurable"]["thread_id"]

        # ä½¿ç”¨æµå¼ç”Ÿæˆ
        full_content = ""
        last_checkpoint_time = time.time()

        async for chunk in self.llm.stream_generate(topic):
            full_content += chunk

            # æ¯ 3 ç§’ä¿å­˜ä¸€æ¬¡ä¸­é—´æ€
            current_time = time.time()
            if current_time - last_checkpoint_time >= self.checkpoint_saver.save_interval:
                await self.checkpoint_saver.save_intermediate(
                    thread_id=thread_id,
                    node_name="content_agent",
                    partial_state={
                        "channel_values": {
                            "generated_content": full_content,
                            "topic": topic
                        }
                    }
                )
                last_checkpoint_time = current_time

        # å®Œæˆåæœ€ç»ˆä¿å­˜
        await self.checkpoint_saver.finalize_checkpoint(
            thread_id=thread_id,
            final_state={
                "channel_values": {
                    "generated_content": full_content,
                    "topic": topic
                }
            }
        )

        return {"content": full_content}
```

**å‰ç«¯æ¢å¤é€»è¾‘**ï¼š

```typescript
// frontend/hooks/useStreamingRecovery.ts

export function useStreamingRecovery(threadId: string, documentId: string) {
  const [streamStatus, setStreamStatus] = useState<{
    status: 'idle' | 'streaming' | 'interrupted' | 'completed'
    checkpointId: string | null
  }>({
    status: 'idle',
    checkpointId: null
  });

  // è½®è¯¢æ£€æŸ¥ç‚¹çŠ¶æ€
  useEffect(() => {
    const interval = setInterval(async () => {
      if (streamStatus.status === 'streaming' || streamStatus.status === 'interrupted') {
        const response = await fetch(
          `/api/sessions/${threadId}/checkpoint/latest`
        );
        const checkpoint = await response.json();

        if (checkpoint.metadata?.is_partial) {
          setStreamStatus({
            status: 'interrupted',
            checkpointId: checkpoint.id
          });
        } else if (checkpoint.metadata?.is_partial === false) {
          setStreamStatus({
            status: 'completed',
            checkpointId: null
          });
        }
      }
    }, 2000);  // æ¯ 2 ç§’æ£€æŸ¥ä¸€æ¬¡

    return () => clearInterval(interval);
  }, [threadId, documentId]);

  const resumeFromCheckpoint = async () => {
    if (!streamStatus.checkpointId) return;

    await fetch(`/api/sessions/${threadId}/resume/${streamStatus.checkpointId}`, {
      method: 'POST'
    });

    setStreamStatus({ status: 'streaming', checkpointId: null });
  };

  return { streamStatus, resumeFromCheckpoint };
}
```

---

## å†²çªä¸ƒï¼šæ„å›¾åˆ†ç±»è¿‡åº¦ (ğŸŸ¢ ä½)

### é—®é¢˜æè¿°

```
"æˆ‘æƒ³å­¦ React" â”€â”€â”€â”€â”€æ­£åˆ™åŒ¹é…â”€â”€â”€â”€> new_topic (0ms)
     â†“
     â”€â”€â”€â”€â”€LLM ç¡®è®¤â”€â”€â”€â”€> (1-2s)
```

**é£é™©**ï¼šç®€å•åœºæ™¯è¢«å»¶è¿Ÿã€‚

### è§£å†³æ–¹æ¡ˆï¼šåˆ†å±‚åŒ¹é…ç­–ç•¥

```python
# backend/agent/intent_classifier.py

class IntentClassifier:
    """
    åˆ†å±‚æ„å›¾åˆ†ç±»å™¨

    ç¬¬ 1 å±‚ï¼šå¼ºè§„åˆ™åŒ¹é… (0-5ms)
    ç¬¬ 2 å±‚ï¼šæ¨¡ç³Šè§„åˆ™åŒ¹é… (5-10ms)
    ç¬¬ 3 å±‚ï¼šLLM åˆ†ç±» (500-2000ms)
    """

    def __init__(self):
        self.strong_patterns = {
            r"æˆ‘æƒ³å­¦|æˆ‘æƒ³äº†è§£|æ•™æ•™æˆ‘|ä»€ä¹ˆæ˜¯": ("new_topic", 1.0),
            r"è¯¦ç»†è¯´è¯´|æ·±å…¥è®²è®²|å†è¯¦ç»†ç‚¹": ("follow_up", 1.0),
            r"å’Œ.*çš„åŒºåˆ«|å’Œ.*ä¸åŒ|å¯¹æ¯”": ("comparison", 1.0),
            r"æ€ä¹ˆåŠ|æ€ä¹ˆåš|å¦‚ä½•å®ç°": ("question_practical", 1.0),
        }
        self.fuzzy_patterns = {
            "è®²è¯¦ç»†": "follow_up",
            "è¯´æ¸…æ¥š": "optimize_request",
            "ä¸¾ä¾‹": "optimize_request",
            "æ›´æ·±å…¥": "follow_up"
        }

    async def classify(self, message: str, context: dict) -> dict:
        """
        åˆ†å±‚åˆ†ç±»
        """

        # === ç¬¬ 1 å±‚ï¼šå¼ºè§„åˆ™åŒ¹é… ===
        for pattern, (intent, confidence) in self.strong_patterns.items():
            if re.match(pattern, message):
                return {
                    "intent_type": intent,
                    "confidence": confidence,
                    "method": "strong_rule",
                    "processing_time_ms": 5
                }

        # === ç¬¬ 2 å±‚ï¼šæ¨¡ç³ŠåŒ¹é… ===
        fuzzy_match = self._fuzzy_match(message)
        if fuzzy_match:
            return {
                "intent_type": fuzzy_match,
                "confidence": 0.8,
                "method": "fuzzy_rule",
                "processing_time_ms": 10
            }

        # === ç¬¬ 3 å±‚ï¼šLLM åˆ†ç±» ===
        if context.get("use_llm", True):
            return await self._llm_classify(message, context)

    def _fuzzy_match(self, message: str) -> str:
        """æ¨¡ç³ŠåŒ¹é…"""

        words = message.split()
        for keyword, intent in self.fuzzy_patterns.items():
            if keyword in words:
                return intent
        return None

    async def _llm_classify(self, message: str, context: dict):
        """LLM åˆ†ç±»"""

        prompt = f"""
åˆ†æç”¨æˆ·æ¶ˆæ¯æ„å›¾ï¼ˆä»…å½“æ— æ³•è§„åˆ™åŒ¹é…æ—¶ä½¿ç”¨ï¼‰ï¼š

ã€ç”¨æˆ·æ¶ˆæ¯ã€‘{message}

ã€ä¼šè¯ä¸Šä¸‹æ–‡ã€‘
å½“å‰æ–‡æ¡£: {context.get('current_doc')}
æœ€è¿‘å­¦ä¹ : {context.get('recent_topics', [])}

å¿«é€Ÿåˆ¤æ–­å¹¶è¿”å› JSONï¼š
{{
  "intent_type": "...",
  "confidence": 0.95
}}
"""

        result = await self.llm.generate_json(prompt)
        result["method"] = "llm"
        result["processing_time_ms"] = 1200  # ä¼°è®¡å€¼
        return result
```

---

## å†²çªå…«ï¼šç”¨æˆ·çŠ¶æ€ä¸ä¸€è‡´ (ğŸŸ¡ ä¸­)

### é—®é¢˜æè¿°

```
æ€»ç»“å™¨åˆ¤æ–­: user_level = "advanced"
     â†“
Planner Agent åˆ¤æ–­: user_level = "beginner"
     â†“
ç”Ÿæˆå†…å®¹çŸ›ç›¾
```

**é£é™©**ï¼šAI è¡Œä¸ºä¸ä¸€è‡´ï¼Œç”¨æˆ·ä½“éªŒæ··ä¹±ã€‚

### è§£å†³æ–¹æ¡ˆï¼šç»Ÿä¸€ç”¨æˆ·ç”»åƒç³»ç»Ÿ

```python
# backend/services/user_profile.py

class UserProfileManager:
    """
    ç»Ÿä¸€ç”¨æˆ·ç”»åƒç®¡ç†

    æ‰€æœ‰ Agent ä»åŒä¸€æ¥æºè·å–ç”¨æˆ·ä¿¡æ¯
    """

    def __init__(self, db):
        self.db = db
        self.cache = {}  # session_id -> profile

    async def get_profile(self, session_id: str) -> dict:
        """è·å–ç”¨æˆ·ç”»åƒ"""

        if session_id in self.cache:
            return self.cache[session_id]

        # 1. è·å–ä¼šè¯ä¿¡æ¯
        session = await self.db.get_session(session_id)

        # 2. è®¡ç®—ç”¨æˆ·æ°´å¹³
        user_level = await self._calculate_level(session)

        # 3. åˆ†æå­¦ä¹ é£æ ¼
        learning_style = await self._analyze_style(session)

        # 4. æå–åå¥½ä¸»é¢˜
        preferred_topics = await self._extract_preferences(session)

        profile = {
            "user_level": user_level,
            "learning_style": learning_style,
            "preferred_topics": preferred_topics,
            "session_stats": {
                "total_docs": await self.db.count_documents(session_id),
                "completed_docs": await self.db.count_completed(session_id),
                "avg_engagement": await self._calculate_engagement(session)
            }
        }

        self.cache[session_id] = profile
        return profile

    async def _calculate_level(self, session: dict) -> str:
        """
        è®¡ç®—ç”¨æˆ·æ°´å¹³

        ç»¼åˆè€ƒè™‘ï¼šæ–‡æ¡£æ•°é‡ã€å®Œæˆåº¦ã€äº’åŠ¨è´¨é‡
        """

        stats = await self._get_learning_stats(session)

        # å¤šç»´åº¦è¯„åˆ†
        depth_score = min(stats["unique_topics_count"] / 20, 1.0)
        completion_score = stats["completion_rate"]
        engagement_score = stats["avg_interaction_quality"]

        overall_score = (
            depth_score * 0.3 +
            completion_score * 0.4 +
            engagement_score * 0.3
        )

        if overall_score < 0.3:
            return "beginner"
        elif overall_score < 0.6:
            return "intermediate"
        else:
            return "advanced"

    async def update_interaction(self, session_id: str, interaction: dict):
        """
        æ›´æ–°äº¤äº’æ•°æ®

        æ¯æ¬¡ç”¨æˆ·äº¤äº’åè°ƒç”¨ï¼Œæ›´æ–°ç”»åƒ
        """

        # é‡æ–°è®¡ç®—ç”»åƒ
        await self.invalidate(session_id)
        profile = await self.get_profile(session_id)

        return profile
```

**Agent ä½¿ç”¨**ï¼š

```python
# æ‰€æœ‰ Agent ä» UserProfileManager è·å–ç”¨æˆ·ä¿¡æ¯

async def intent_agent_node(state: AgentState):
    """Intent Agent - ä½¿ç”¨ç»Ÿä¸€ç”»åƒ"""

    profile = await user_profile_manager.get_profile(state["session_id"])

    # ä½¿ç”¨ç»Ÿä¸€ç”»åƒè¿›è¡Œæ„å›¾åˆ†æ
    intent = await llm.classify(
        message=state["raw_message"],
        user_level=profile["user_level"],
        learning_style=profile["learning_style"]
    )

    return {**state, "intent": intent}
```

---

## å†²çªä¹ï¼šé”šç‚¹å¤±æ•ˆ (ğŸ”´ é«˜)

### é—®é¢˜æè¿°

```
æ–‡æ¡£æ›´æ–°åå­—ç¬¦åç§»å˜åŒ–
     â†“
è¯„è®ºçš„é”šç‚¹å¤±æ•ˆ
     â†“
ç”¨æˆ·æ‰¾ä¸åˆ°è¯„è®ºä½ç½®
```

**é£é™©**ï¼šç”¨æˆ·æ ‡æ³¨ä¸¢å¤±ã€‚

### è§£å†³æ–¹æ¡ˆï¼šå†…å®¹æŒ‡çº¹é”šç‚¹

```python
# backend/models/semantic_anchors.py

class ContentFingerprintAnchor:
    """
    å†…å®¹æŒ‡çº¹é”šç‚¹ç³»ç»Ÿ

    ä¸ä¾èµ–å­—ç¬¦ä½ç½®ï¼Œä½¿ç”¨å†…å®¹æŒ‡çº¹
    """

    def create_anchor(self, content: str, context: dict) -> str:
        """åˆ›å»ºç¨³å®šçš„é”šç‚¹"""

        # 1. æå–å†…å®¹æŒ‡çº¹
        fingerprint = self._generate_fingerprint(content)

        # 2. ç”Ÿæˆé”šç‚¹ ID
        anchor_id = f"anchor-{fingerprint}"

        return {
            "anchor_id": anchor_id,
            "fingerprint": fingerprint,
            "original_content": content
        }

    def _generate_fingerprint(self, content: str) -> str:
        """
        ç”Ÿæˆå†…å®¹æŒ‡çº¹

        ä½¿ç”¨å¤šç§æ–¹æ³•çš„ç»„åˆ
        """

        # æ–¹æ³• 1: å†…å®¹å“ˆå¸Œ
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]

        # æ–¹æ³• 2: å…³é”®è¯æå–
        keywords = self._extract_keywords(content)
        keyword_hash = "-".join(sorted(keywords))

        # æ–¹æ³• 3: ç»“æ„æŒ‡çº¹
        structure_hash = self._analyze_structure(content)

        # ç»„åˆæŒ‡çº¹
        return f"{content_hash}:{keyword_hash}:{structure_hash}"

    def locate_anchor(
        self, document: dict, anchor_id: str
    ) -> dict:
        """
        åœ¨æ–‡æ¡£ä¸­å®šä½é”šç‚¹

        å³ä½¿æ–‡æ¡£è¢«æ›´æ–°ï¼Œä¹Ÿèƒ½æ‰¾åˆ°
        """

        fingerprint = anchor_id.split("-")[1] if "-" in anchor_id else ""

        # 1. ç²¾ç¡®åŒ¹é…
        for section in document["sections"]:
            if self._fingerprint_matches(section["content"], fingerprint):
                return {
                    "section_id": section["id"],
                    "match_type": "exact",
                    "confidence": 1.0
                }

        # 2. æ¨¡ç³ŠåŒ¹é…
        best_match = None
        best_score = 0

        for section in document["sections"]:
            score = self._similarity_score(section["content"], fingerprint)
            if score > best_score:
                best_score = score
                best_match = {
                    "section_id": section["id"],
                    "match_type": "fuzzy",
                    "confidence": score
                }

        if best_match and best_score > 0.6:
            return best_match

        return None

    def _fingerprint_matches(self, content: str, fingerprint: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ¹é…æŒ‡çº¹"""

        # è§£ææŒ‡çº¹
        parts = fingerprint.split(":")
        if len(parts) < 3:
            return False

        content_hash, keyword_hash, structure_hash = parts

        # éªŒè¯å„ä¸ªéƒ¨åˆ†
        current_content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
        if current_content_hash != content_hash:
            return False

        # éªŒè¯å…³é”®è¯
        current_keywords = self._extract_keywords(content)
        current_keyword_hash = "-".join(sorted(current_keywords))
        if current_keyword_hash != keyword_hash:
            # å…³é”®è¯ç›¸ä¼¼åº¦æ£€æŸ¥
            similarity = self._keyword_similarity(current_keywords, keyword_hash.split("-"))
            if similarity < 0.7:
                return False

        return True
```

**é…åˆä½¿ç”¨**ï¼š

```python
# ä¿å­˜è¯„è®ºæ—¶è®°å½•å†…å®¹æŒ‡çº¹

comment = await db.save_comment({
    "document_id": doc_id,
    "user_comment": "è¿™é‡Œå¤ªæŠ½è±¡äº†",
    "anchor_id": content_fingerprint.create_anchor(
        content=selected_text,
        context={"section_id": section_id}
    )["anchor_id"]
})

# æ–‡æ¡£æ›´æ–°åï¼Œä»ç„¶å¯ä»¥é€šè¿‡æŒ‡çº¹æ‰¾åˆ°é”šç‚¹
```

---

## å†²çªåï¼šå‘é‡åŒæ­¥ç¼ºå¤± (ğŸŸ¢ ä½)

### é—®é¢˜æè¿°

```
Route Agent æœç´¢ç›¸ä¼¼æ–‡æ¡£
     â†“
å‘é‡ç´¢å¼•ä¸ SQLite ä¸åŒæ­¥
     â†“
æ‰¾ä¸åˆ°åˆšç”Ÿæˆçš„æ–‡æ¡£
```

**é£é™©**ï¼šåŠŸèƒ½ä¸å¯é ã€‚

### è§£å†³æ–¹æ¡ˆï¼šå‘é‡åŒæ­¥ç­–ç•¥

```python
# backend/services/vector_sync.py

class VectorSynchronizer:
    """
    å‘é‡åŒæ­¥å™¨

    ç¡®ä¿ SQLite documents è¡¨ä¸å‘é‡ç´¢å¼•ä¿æŒåŒæ­¥
    """

    def __init__(self, db, vector_client):
        self.db = db
        self.vector_client = vector_client
        self.sync_lock = asyncio.Lock()

    async def sync_document(self, document_id: int):
        """
        åŒæ­¥å•ä¸ªæ–‡æ¡£åˆ°å‘é‡åº“

        æ–‡æ¡£åˆ›å»º/æ›´æ–°åè°ƒç”¨
        """

        async with self.sync_lock:
            # 1. è·å–æ–‡æ¡£
            document = await self.db.get_document(document_id)

            if not document:
                return

            # 2. ç”Ÿæˆå‘é‡åµŒå…¥
            chunks = self._chunk_document(document)
            embeddings = await self._generate_embeddings(chunks)

            # 3. æ›´æ–°å‘é‡åº“
            await self._upsert_vectors(document, chunks, embeddings)

    async def sync_batch(self, limit: int = 100):
        """
        æ‰¹é‡åŒæ­¥

        å®šæœŸæ‰§è¡Œï¼Œç¡®ä¿ä¸€è‡´æ€§
        """

        async with self.sync_lock:
            # 1. æ‰¾å‡ºéœ€è¦åŒæ­¥çš„æ–‡æ¡£
            docs_to_sync = await self.db.get_unsynced_documents(limit)

            # 2. æ‰¹é‡åŒæ­¥
            for doc in docs_to_sync:
                await self.sync_document(doc["id"])

            # 3. æ ‡è®°ä¸ºå·²åŒæ­¥
            await self.db.mark_as_synced([d["id"] for d in docs_to_sync])

    def _chunk_document(self, document: dict) -> list:
        """
        æ–‡æ¡£åˆ†å—

        ç”¨äºå‘é‡åŒ–
        """

        content = document["content"]

        # æŒ‰ç« èŠ‚åˆ†å—
        sections = self._split_by_sections(content)

        # ç¡®ä¿æ¯å—ä¸è¶…è¿‡ token é™åˆ¶
        chunks = []
        for section in sections:
            if len(section) > 500:
                sub_chunks = self._split_by_tokens(section, max_tokens=500)
                chunks.extend(sub_chunks)
            else:
                chunks.append(section)

        return chunks

    async def _generate_embeddings(self, chunks: list) -> list:
        """ç”ŸæˆåµŒå…¥å‘é‡"""

        # è°ƒç”¨åµŒå…¥æ¨¡å‹
        embeddings = []
        for chunk in chunks:
            embedding = await self.embedding_model.embed(chunk)
            embeddings.append({
                "text": chunk,
                "vector": embedding,
                "dimension": len(embedding)
            })

        return embeddings

    async def _upsert_vectors(self, document, chunks, embeddings):
        """æ›´æ–°å‘é‡åº“"""

        # å‡†å¤‡å‘é‡æ•°æ®
        vectors = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            vectors.append({
                "id": f"{document['id']}-{i}",
                "document_id": document["id"],
                "text": chunk,
                "vector": embedding["vector"],
                "metadata": {
                    "topic": document["topic"],
                    "category_path": document["category_path"],
                    "chunk_index": i
                }
            })

        # æ‰¹é‡ upsert
        await self.vector_client.upsert(vectors)
```

**åŒæ­¥è§¦å‘ç‚¹**ï¼š

```python
# åœ¨è¿™äº›æ—¶æœºè§¦å‘åŒæ­¥

# 1. æ–‡æ¡£ç”Ÿæˆå
async def content_agent_node(state):
    result = await generate_document(...)
    await vector_sync.sync_document(result["document"]["id"])
    return result

# 2. å®šæœŸåŒæ­¥
@router.post("/admin/sync-vectors")
async def manual_sync():
    """æ‰‹åŠ¨è§¦å‘å‘é‡åŒæ­¥"""
    await vector_sync.sync_batch(limit=1000)
    return {"status": "synced"}

# 3. åå°ä»»åŠ¡
@background_task.schedule(hourly)
async def periodic_vector_sync():
    """æ¯å°æ—¶åŒæ­¥ä¸€æ¬¡"""
    await vector_sync.sync_batch()
```

---

## ä¼˜å…ˆçº§å®æ–½è®¡åˆ’

### P0 (ç«‹å³ä¿®å¤)

```
â–¡ å†²çªä¸€ï¼šç»Ÿä¸€ v2 æ¶æ„
    - ç§»é™¤ v1 ç®€å•è·¯ç”±
    - æ‰€æœ‰è¾“å…¥ä½¿ç”¨ Input Normalizer
    - é¢„è®¡å·¥ä½œé‡ï¼š2-3 å¤©

â–¡ å†²çªäºŒï¼šCheckpoint åˆ†å±‚å­˜å‚¨
    - å®ç° LayeredCheckpointSaver
    - é¢„è®¡å·¥ä½œé‡ï¼š3-5 å¤©

â–¡ å†²çªå…­ï¼šæµå¼ä¸­é—´æ€ä¿å­˜
    - å®ç° StreamingCheckpointSaver
    - é¢„è®¡å·¥ä½œé‡ï¼š2-3 å¤©
```

### P1 (é‡è¦ä½†å¯å»¶å)

```
â–¡ å†²çªä¹ï¼šè¯­ä¹‰é”šç‚¹
    - å®ç° ContentFingerprintAnchor
    - é¢„è®¡å·¥ä½œé‡ï¼š5-7 å¤©

â–¡ å†²çªäº”ï¼šåˆ†ç±» Schema
    - å®ç° TaxonomySchema
    - é¢„è®¡å·¥ä½œé‡ï¼š3-4 å¤©

â–¡ å†²çªå…«ï¼šç»Ÿä¸€ç”¨æˆ·ç”»åƒ
    - å®ç° UserProfileManager
    - é¢„è®¡å·¥ä½œé‡ï¼š3-5 å¤©
```

### P2 (ä¼˜åŒ–é¡¹)

```
â–¡ å†²çªä¸‰ï¼šå®ä½“è¯ç´¢å¼•
    - å®Œæ•´å®ç° EntityIndex
    - é¢„è®¡å·¥ä½œé‡ï¼š5-7 å¤©

â–¡ å†²çªå››ï¼šå¢é‡æ›´æ–°ç³»ç»Ÿ
    - å®ç° DocumentUpdater
    - é¢„è®¡å·¥ä½œé‡ï¼š5-7 å¤©

â–¡ å†²çªä¸ƒï¼šåˆ†å±‚æ„å›¾åˆ†ç±»
    - å®ç° IntentClassifier
    - é¢„è®¡å·¥ä½œé‡ï¼š2-3 å¤©

â–¡ å†²çªåï¼šå‘é‡åŒæ­¥
    - å®ç° VectorSynchronizer
    - é¢„è®¡å·¥ä½œé‡ï¼š5-7 å¤©
```

---

*è®¾è®¡å†²çªåˆ†æä¸è§£å†³æ–¹æ¡ˆ v1.0 | KnowZero é¡¹ç›®*
